{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is for classifies references into topics on the entire references csv file. To do this, we:\n",
    "\n",
    "* Used a 'BART' model to give each reference a confidence score for each topic. Updated .csv files are saved at every 100,000 rows\n",
    "\n",
    "* All .csv files are combined and saved as 'combined_results.csv'\n",
    "\n",
    "* Top 35% of confidence scores for each topic are classified as 1, the rest are classified as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertbecker/miniconda3/envs/my_project_env/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/11 already processed. Skipping rows 0 to 10000.\n",
      "Chunk 2/11 already processed. Skipping rows 10000 to 20000.\n",
      "Chunk 3/11 already processed. Skipping rows 20000 to 30000.\n",
      "Chunk 4/11 already processed. Skipping rows 30000 to 40000.\n",
      "Chunk 5/11 already processed. Skipping rows 40000 to 50000.\n",
      "Chunk 6/11 already processed. Skipping rows 50000 to 60000.\n",
      "Chunk 7/11 already processed. Skipping rows 60000 to 70000.\n",
      "Chunk 8/11 already processed. Skipping rows 70000 to 80000.\n",
      "Processing chunk 9/11, rows 80000 to 90000 at 10:29:10\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Chunk saved to 'results/results_90000.csv' with 10000 rows.\n",
      "Processing chunk 10/11, rows 90000 to 100000 at 11:03:40\n",
      "Chunk saved to 'results/results_100000.csv' with 10000 rows.\n",
      "Processing chunk 11/11, rows 100000 to 109691 at 12:11:56\n",
      "Chunk saved to 'results/results_109691.csv' with 9691 rows.\n",
      "All chunks processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "def process_dataset_in_chunks(df, categories, batch_size=25, chunk_size=10000, results_dir=\"results\"):\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=\"cross-encoder/nli-distilroberta-base\",\n",
    "        device=-1\n",
    "    )\n",
    "\n",
    "    total_rows = len(df)\n",
    "    num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, total_rows)\n",
    "\n",
    "        output_file = os.path.join(results_dir, f\"results_{end_idx}.csv\")\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Chunk {chunk_idx + 1}/{num_chunks} already processed. Skipping rows {start_idx} to {end_idx}.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing chunk {chunk_idx + 1}/{num_chunks}, rows {start_idx} to {end_idx} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        chunk_df = df.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "        category_scores = {cat: [] for cat in categories}\n",
    "\n",
    "        for i in range(0, len(chunk_df), batch_size):\n",
    "            batch_texts = chunk_df['context'].iloc[i:i+batch_size]\n",
    "            \n",
    "            for text in batch_texts:\n",
    "                try:\n",
    "                    if not isinstance(text, str) or not text:\n",
    "                        for cat in categories:\n",
    "                            category_scores[cat].append(0.0)\n",
    "                        continue\n",
    "\n",
    "                    result = classifier(text, candidate_labels=categories)\n",
    "                    scores_dict = dict(zip(result['labels'], result['scores']))\n",
    "                    \n",
    "                    for cat in categories:\n",
    "                        category_scores[cat].append(round(scores_dict.get(cat, 0.0), 3))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing text: {str(e)}\")\n",
    "                    for cat in categories:\n",
    "                        category_scores[cat].append(0.0)\n",
    "\n",
    "        for cat in categories:\n",
    "            chunk_df[f'confidence_{cat}'] = category_scores[cat]\n",
    "\n",
    "        chunk_df.to_csv(output_file, index=False)\n",
    "        print(f\"Chunk saved to '{output_file}' with {len(chunk_df)} rows.\")\n",
    "\n",
    "    print(\"All chunks processed and saved.\")\n",
    "\n",
    "categories = [\n",
    "    \"politics\", \"ethics\", \"epistemology\", \"logic\",\n",
    "    \"metaphysics\", \"science\", \"religion\",\n",
    "]\n",
    "\n",
    "df = pd.read_csv('references.csv')\n",
    "\n",
    "process_dataset_in_chunks(\n",
    "    df,\n",
    "    categories,\n",
    "    batch_size=25,\n",
    "    chunk_size=10000,\n",
    "    results_dir=\"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chunks have been combined and saved to combined_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Combine chunks\n",
    "results_dir = \"results\"\n",
    "dataframes = []\n",
    "\n",
    "for file in os.listdir(results_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(results_dir, file)\n",
    "        chunk_df = pd.read_csv(file_path)\n",
    "        dataframes.append(chunk_df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "output_file = \"combined_results.csv\"\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print(f\"All chunks have been combined and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('combined_results.csv')\n",
    "\n",
    "confidence_cols = [col for col in df.columns if col.startswith('confidence_')]\n",
    "binary_df = df.copy()\n",
    "\n",
    "for col in confidence_cols:\n",
    "    threshold = df[col].quantile(0.65)\n",
    "    binary_df[col] = (binary_df[col] >= threshold).astype(int)\n",
    "\n",
    "all_zeros = binary_df[confidence_cols].sum(axis=1) == 0\n",
    "if all_zeros.any():\n",
    "    for idx in binary_df[all_zeros].index:\n",
    "        best_topic = df.loc[idx, confidence_cols].idxmax()\n",
    "        binary_df.loc[idx, best_topic] = 1\n",
    "\n",
    "print(\"\\nBinary Classification Results:\")\n",
    "print(f\"Total rows: {len(binary_df)}\")\n",
    "print(f\"Rows with all zeros before fixing: {all_zeros.sum()}\")\n",
    "print(\"\\nDistribution for each topic:\")\n",
    "for col in confidence_cols:\n",
    "    ones = binary_df[col].sum()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Ones: {ones} ({(ones/len(binary_df))*100:.1f}%)\")\n",
    "    print(f\"Zeros: {len(binary_df)-ones} ({((len(binary_df)-ones)/len(binary_df))*100:.1f}%)\")\n",
    "\n",
    "binary_df.to_csv('binary_results.csv', index=False)\n",
    "print(\"\\nBinary results saved to 'binary_results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
