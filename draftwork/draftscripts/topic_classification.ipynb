{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertbecker/miniconda3/envs/my_project_env/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/11 already processed. Skipping rows 0 to 10000.\n",
      "Chunk 2/11 already processed. Skipping rows 10000 to 20000.\n",
      "Chunk 3/11 already processed. Skipping rows 20000 to 30000.\n",
      "Chunk 4/11 already processed. Skipping rows 30000 to 40000.\n",
      "Chunk 5/11 already processed. Skipping rows 40000 to 50000.\n",
      "Chunk 6/11 already processed. Skipping rows 50000 to 60000.\n",
      "Chunk 7/11 already processed. Skipping rows 60000 to 70000.\n",
      "Chunk 8/11 already processed. Skipping rows 70000 to 80000.\n",
      "Processing chunk 9/11, rows 80000 to 90000 at 10:29:10\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Chunk saved to 'results/results_90000.csv' with 10000 rows.\n",
      "Processing chunk 10/11, rows 90000 to 100000 at 11:03:40\n",
      "Chunk saved to 'results/results_100000.csv' with 10000 rows.\n",
      "Processing chunk 11/11, rows 100000 to 109691 at 12:11:56\n",
      "Chunk saved to 'results/results_109691.csv' with 9691 rows.\n",
      "All chunks processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def process_dataset_in_chunks(df, categories, batch_size=25, chunk_size=10000, threshold=0.5, results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Process a large dataset in chunks, saving results after each chunk and resuming from the last completed chunk.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing 'context' column\n",
    "    categories (list): List of categories to classify into\n",
    "    batch_size (int): Number of texts to process per batch\n",
    "    chunk_size (int): Number of rows to process per chunk\n",
    "    threshold (float): Confidence threshold for top 2 topics\n",
    "    results_dir (str): Directory to save results\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=\"cross-encoder/nli-distilroberta-base\",\n",
    "        device=-1\n",
    "    )\n",
    "\n",
    "    total_rows = len(df)\n",
    "    num_chunks = (total_rows + chunk_size - 1) // chunk_size  #  number of chunks to process\n",
    "\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, total_rows)\n",
    "\n",
    "        # skip if chunk has already been processed\n",
    "        output_file = os.path.join(results_dir, f\"results_{end_idx}.csv\")\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Chunk {chunk_idx + 1}/{num_chunks} already processed. Skipping rows {start_idx} to {end_idx}.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing chunk {chunk_idx + 1}/{num_chunks}, rows {start_idx} to {end_idx} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        chunk_df = df.iloc[start_idx:end_idx].copy()  # work on a copy of the data\n",
    "\n",
    "        all_predictions = []\n",
    "        all_confidences = []\n",
    "        error_count = 0\n",
    "\n",
    "        # process in batches\n",
    "        for i in range(0, len(chunk_df), batch_size):\n",
    "            batch_texts = chunk_df['context'].iloc[i:i+batch_size]\n",
    "            batch_predictions = []\n",
    "            batch_confidences = []\n",
    "\n",
    "            for text in batch_texts:\n",
    "                try:\n",
    "                    truncated_text = text[:80] if isinstance(text, str) else \"\"\n",
    "\n",
    "                    if not truncated_text:\n",
    "                        batch_predictions.append(\"unknown\")\n",
    "                        batch_confidences.append(0.0)\n",
    "                        continue\n",
    "\n",
    "                    # classification\n",
    "                    result = classifier(truncated_text, candidate_labels=categories)\n",
    "\n",
    "                    # Get top 2 labels and scores\n",
    "                    top_labels = result['labels'][:2]\n",
    "                    top_scores = result['scores'][:2]\n",
    "                    combined_confidence = sum(top_scores)\n",
    "\n",
    "                    # check confidence threshold to determine if we add a third topic\n",
    "                    if combined_confidence < threshold:\n",
    "                        top_labels.append(result['labels'][2])\n",
    "\n",
    "                    batch_predictions.append(\", \".join(top_labels))\n",
    "                    batch_confidences.append(round(combined_confidence, 3))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing text: {str(e)}\")\n",
    "                    batch_predictions.append(\"error\")\n",
    "                    batch_confidences.append(0.0)\n",
    "                    error_count += 1\n",
    "\n",
    "            all_predictions.extend(batch_predictions)\n",
    "            all_confidences.extend(batch_confidences)\n",
    "\n",
    "        # Save chunk results\n",
    "        chunk_df['predicted_category'] = all_predictions\n",
    "        chunk_df['confidence'] = all_confidences\n",
    "        chunk_df.to_csv(output_file, index=False)\n",
    "        print(f\"Chunk saved to '{output_file}' with {len(chunk_df)} rows.\")\n",
    "\n",
    "    print(\"All chunks processed and saved.\")\n",
    "\n",
    "# define categories\n",
    "categories = [\n",
    "    \"politics\", \"ethics\", \"epistemology\", \"logic\",\n",
    "    \"art\", \"metaphysics\", \"science\", \"language\",\n",
    "]\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('references.csv')\n",
    "\n",
    "process_dataset_in_chunks(\n",
    "    df,\n",
    "    categories,\n",
    "    batch_size=25,  # good batch for Mac M3 chip\n",
    "    chunk_size=10000,  # chunks are rows of 10,000\n",
    "    threshold=0.5,  # confidence threshold\n",
    "    results_dir=\"results\"  # save each chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chunks have been combined and saved to combined_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Combine chunks\n",
    "results_dir = \"results\"\n",
    "dataframes = []\n",
    "\n",
    "for file in os.listdir(results_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(results_dir, file)\n",
    "        chunk_df = pd.read_csv(file_path)\n",
    "        dataframes.append(chunk_df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "output_file = \"combined_results.csv\"\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print(f\"All chunks have been combined and saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
